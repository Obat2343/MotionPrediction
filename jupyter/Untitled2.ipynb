{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kornia requires version >= 3.6. your version 3.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "import progressbar\n",
    "import numpy as np\n",
    "import datetime\n",
    "import shutil\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"../git/future-image-similarity\")\n",
    "\n",
    "import utils\n",
    "\n",
    "sys.path.append('../')\n",
    "from pycode.dataset import RLBench_dataset3\n",
    "from pycode.config import _C as cfg\n",
    "from pycode.misc import save_outputs, build_model_MP, build_dataset_MP, build_optimizer, str2bool, save_args, save_checkpoint, load_checkpoint, Timer, Time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--beta1', default=0.9, type=float, help='momentum term for adam')\n",
    "parser.add_argument('--log_dir', default='logs', help='base directory to save logs')\n",
    "parser.add_argument('--model_dir', default='', help='base directory to save trained models')\n",
    "parser.add_argument('--name', default='', help='identifier for directory')\n",
    "parser.add_argument('--data_root', default='data', help='root directory for data')\n",
    "parser.add_argument('--niter', type=int, default=100, help='number of epochs to train for')\n",
    "parser.add_argument('--seed', default=1, type=int, help='manual seed')\n",
    "parser.add_argument('--epoch_size', type=int, default=600, help='epoch size')\n",
    "parser.add_argument('--image_width', type=int, default=256, help='the height / width of the input image to network')\n",
    "parser.add_argument('--channels', default=3, type=int)\n",
    "parser.add_argument('--dataset', default='lab_pose', help='predictor training data: lab_pose or gaz_pose')\n",
    "parser.add_argument('--rnn_size', type=int, default=256, help='dimensionality of hidden layer')\n",
    "parser.add_argument('--prior_rnn_layers', type=int, default=1, help='number of layers')\n",
    "parser.add_argument('--posterior_rnn_layers', type=int, default=1, help='number of layers')\n",
    "parser.add_argument('--z_dim', type=int, default=64, help='dimensionality of z_t')\n",
    "parser.add_argument('--g_dim', type=int, default=128, help='dimensionality of encoder output vector and decoder input vector')\n",
    "parser.add_argument('--beta', type=float, default=0.0001, help='weighting on KL to prior')\n",
    "parser.add_argument('--data_threads', type=int, default=5, help='number of data loading threads')\n",
    "parser.add_argument('--last_frame_skip', action='store_true', help='if true, skip connections go between frame t and frame t+t rather than last ground truth frame')\n",
    "\n",
    "parser.add_argument('--config_file', type=str, default='', metavar='FILE', help='path to config file')\n",
    "parser.add_argument('--output_dirname', type=str, default='', help='')\n",
    "parser.add_argument('--log_step', type=int, default=100, help='')\n",
    "parser.add_argument('--save_step', type=int, default=10000, help='')\n",
    "parser.add_argument('--eval_step', type=int, default=5000, help='')\n",
    "parser.add_argument('--log2wandb', type=str2bool, default=True)\n",
    "parser.add_argument('--wandb_group', type=str, default='') # e.g. compare_input\n",
    "parser.add_argument('--save_dataset', type=str2bool, default=False)\n",
    "parser.add_argument('--checkpoint_path', type=str, default=None, help='')\n",
    "\n",
    "args = parser.parse_args([\"--config_file\",\"../configs/RLBench_MBBC.yaml\",\"--log2wandb\",\"False\",\"--output_dirname\",\"hoge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configration file ../configs/RLBench_MBBC.yaml\n",
      "The specified output dir is already exists. Overwrite? y or n: y\n"
     ]
    }
   ],
   "source": [
    "# args.skip_factor = 10\n",
    "# args.n_past = 5 -> use cfg.PAST_LEN\n",
    "\n",
    "# if args.model_dir != '':\n",
    "#     # load model and continue training from checkpoint\n",
    "#     saved_model = torch.load('%s/model.pth' % args.model_dir)\n",
    "        \n",
    "#     optimizer = args.optimizer\n",
    "#     model_dir = args.model_dir\n",
    "#     opt = saved_model['opt']\n",
    "#     args.optimizer = optimizer\n",
    "#     args.model_dir = model_dir\n",
    "#     args.log_dir = '%s/continued' % args.log_dir\n",
    "# else:\n",
    "#     name = 'model_predictor'\n",
    "\n",
    "#     args.log_dir = '%s/%s/%s' % (args.log_dir, args.dataset, name)\n",
    "\n",
    "# os.makedirs('%s/gen/' % args.log_dir, exist_ok=True)\n",
    "\n",
    "# get cfg data\n",
    "if len(args.config_file) > 0:\n",
    "    print('Loaded configration file {}'.format(args.config_file))\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "\n",
    "# define output dirname\n",
    "if len(args.output_dirname) == 0:\n",
    "    dt_now = datetime.datetime.now()\n",
    "    output_dirname = str(dt_now.date()) + '_' + str(dt_now.time())\n",
    "else:\n",
    "    output_dirname = args.output_dirname\n",
    "\n",
    "output_dirname = os.path.join(cfg.BASIC.OUTPUT_DIR, cfg.DATASET.NAME, cfg.DATASET.RLBENCH.TASK_LIST[0], output_dirname)\n",
    "if os.path.exists(output_dirname):\n",
    "    while 1:\n",
    "        ans = input('The specified output dir is already exists. Overwrite? y or n: ')\n",
    "        if ans == 'y':\n",
    "            break\n",
    "        elif ans == 'n':\n",
    "            raise ValueError(\"Please specify correct output dir\")\n",
    "        else:\n",
    "            print('please type y or n')\n",
    "\n",
    "cfg.PAST_LEN = 0\n",
    "cfg.freeze()\n",
    "\n",
    "# define save model path\n",
    "model_path = os.path.join(output_dirname, 'model_log')\n",
    "\n",
    "# make output dir\n",
    "os.makedirs(output_dirname, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# copy config file\n",
    "if len(args.config_file) > 0:\n",
    "    shutil.copy(args.config_file,output_dirname)\n",
    "\n",
    "# save args\n",
    "argsfile_path = os.path.join(output_dirname, \"args.txt\")\n",
    "save_args(args,argsfile_path)\n",
    "\n",
    "# set seed and cuda\n",
    "random.seed(cfg.BASIC.SEED)\n",
    "torch.manual_seed(cfg.BASIC.SEED)\n",
    "torch.cuda.manual_seed_all(cfg.BASIC.SEED)\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(cfg.BASIC.DEVICE)\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# set wandb\n",
    "with open(args.config_file) as file:\n",
    "    obj = yaml.safe_load(file)\n",
    "\n",
    "if args.log2wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    if args.wandb_group == '':\n",
    "        group = None\n",
    "    else:\n",
    "        group = args.wandb_group\n",
    "    run = wandb.init(project='MotionPrediction-{}-{}'.format(cfg.DATASET.NAME, cfg.DATASET.RLBENCH.TASK_LIST[0]), entity='tendon',\n",
    "                    config=obj, save_code=True, name=args.output_dirname, dir=os.path.join(cfg.BASIC.OUTPUT_DIR, cfg.DATASET.NAME),\n",
    "                    group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../output/RLBench3/PickUpCup/model_based_bc_video_pred/model_log/checkpoint_iter100000/\"\n",
    "\n",
    "prior_path = os.path.join(checkpoint_path, \"prior.pth\")\n",
    "encoder_path = os.path.join(checkpoint_path, \"encoder.pth\")\n",
    "decoder_path = os.path.join(checkpoint_path, \"decoder.pth\")\n",
    "pose_network_path = os.path.join(checkpoint_path, \"pose_network.pth\")\n",
    "conv_network_path = os.path.join(checkpoint_path, \"conv_network.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_network(\n",
       "  (pre_lstm): Sequential(\n",
       "    (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_predictor import gaussian_lstm as lstm_model\n",
    "\n",
    "lstm_input_size = int(((args.image_width / 16) - 2)**2 * args.g_dim)\n",
    "lstm_output_size = int(((args.image_width / 16) - 2)**2 * 16)\n",
    "\n",
    "if args.model_dir != '':\n",
    "    posterior = saved_model['posterior']\n",
    "    prior = saved_model['prior']\n",
    "else:\n",
    "    posterior = lstm_model(lstm_input_size, lstm_output_size, args.rnn_size, args.posterior_rnn_layers, cfg.BASIC.BATCH_SIZE)\n",
    "    prior = lstm_model(lstm_input_size, lstm_output_size, args.rnn_size, args.prior_rnn_layers, cfg.BASIC.BATCH_SIZE)\n",
    "\n",
    "    posterior.apply(utils.init_weights)\n",
    "    prior.apply(utils.init_weights)\n",
    "\n",
    "import models.model_predictor as model\n",
    "       \n",
    "if args.model_dir != '':\n",
    "    decoder = saved_model['decoder']\n",
    "    encoder = saved_model['encoder']\n",
    "else:\n",
    "    encoder = model.encoder_conv(args.g_dim, args.channels)\n",
    "    decoder = model.decoder_conv(args.g_dim, args.channels, height=(args.image_width / 16) - 2, width=(args.image_width / 16) - 2)\n",
    "    encoder.apply(utils.init_weights)\n",
    "    decoder.apply(utils.init_weights)\n",
    "\n",
    "pose_network = model.pose_network(16, 14, 14, 13)\n",
    "conv_network = model.conv_network(16+args.g_dim+int(args.z_dim/4), args.g_dim)\n",
    "pose_network.apply(utils.init_weights)\n",
    "conv_network.apply(utils.init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, _, _, _, _ = load_checkpoint(prior, prior_path)\n",
    "encoder, _, _, _, _ = load_checkpoint(encoder, encoder_path)\n",
    "decoder, _, _, _, _ = load_checkpoint(decoder, decoder_path)\n",
    "pose_network, _, _, _, _ = load_checkpoint(pose_network, pose_network_path)\n",
    "conv_network, _, _, _, _ = load_checkpoint(conv_network, conv_network_path)\n",
    "prior, encoder, decoder, pose_network, conv_network, prior.cuda(), encoder.cuda(), decoder.cuda(), pose_network.cuda(), conv_network.cuda()\n",
    "\n",
    "from models.model_value import ModelValue\n",
    "value_network = ModelValue()\n",
    "value_network.apply(utils.init_weights)\n",
    "value_network = value_network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- optimizers ----------------\n",
    "args.optimizer = optim.Adam\n",
    "value_network_optimizer = args.optimizer(value_network.parameters(), lr=args.lr, betas=(args.beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- loss functions -------------------\n",
    "image_criterion = nn.SmoothL1Loss()\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, cfg, device, mode):\n",
    "        super(Loss, self).__init__()\n",
    "        self.loss_dict = {}\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "        self.l1_loss = nn.SmoothL1Loss()\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.loss_dict[\"Model based BC {}/l1 loss value net\".format(self.mode)] = 0\n",
    "        self.loss_dict[\"Model based BC {}/loss value net\".format(self.mode)] = 0\n",
    "        \n",
    "    def get_log(self):\n",
    "        for key in self.loss_dict.keys():\n",
    "            self.loss_dict[key] /= self.count\n",
    "        return self.loss_dict\n",
    "    \n",
    "    def reset_log(self):\n",
    "        self.count = 0\n",
    "        for key in self.loss_dict.keys():\n",
    "            self.loss_dict[key] = 0\n",
    "    \n",
    "    def l1_criterion(self, pred_x, gt_x):\n",
    "        l1_loss = self.l1_loss(pred_x, gt_x * 1000)\n",
    "        self.loss_dict[\"Model based BC {}/l1 loss value net\".format(self.mode)] += l1_loss.item()\n",
    "        self.loss_dict[\"Model based BC {}/loss value net\".format(self.mode)] += l1_loss.item()\n",
    "        return l1_loss\n",
    "\n",
    "train_loss = Loss(cfg, 'cuda', 'train')\n",
    "val_loss = Loss(cfg, 'cuda', 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of future is 1 frame\n",
      "load json data\n",
      "length of future is 1 frame\n",
      "load json data\n"
     ]
    }
   ],
   "source": [
    "# set dataset\n",
    "train_dataset = build_dataset_MP(cfg, save_dataset=args.save_dataset, mode='train')\n",
    "val_dataset = build_dataset_MP(cfg, save_dataset=args.save_dataset, mode='val')\n",
    "\n",
    "# set dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=cfg.BASIC.BATCH_SIZE, shuffle=True, num_workers=cfg.BASIC.WORKERS)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=cfg.BASIC.BATCH_SIZE, shuffle=True, num_workers=cfg.BASIC.WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "import random\n",
    "\n",
    "def make_action(pose,rotation,grasp):\n",
    "    B,S,_ = pose.shape\n",
    "    rotation = rotation.view(B,S,-1)\n",
    "    grasp = torch.unsqueeze(grasp, 2)\n",
    "    \n",
    "    action = torch.cat([pose,rotation,grasp],2)\n",
    "    return action\n",
    "\n",
    "def make_random_action(pose, rotation, grasp):\n",
    "    B, _ = pose.shape\n",
    "    random_pose = [random.uniform(-0.3,0.3) for _ in range(3 * B)]\n",
    "    random_pose = torch.tensor(random_pose, dtype=torch.float32).view(B,3)\n",
    "    random_pose = pose + random_pose.cuda()\n",
    "    \n",
    "    random_angles = [[random.uniform(-30,30) for _ in range(3)] for _ in range(B)]\n",
    "    r = R.from_euler('zyx', random_angles, degrees=True)\n",
    "    random_matrix = torch.tensor(r.as_matrix(), dtype=torch.float32).cuda()\n",
    "    random_matrix = torch.bmm(random_matrix, rotation)\n",
    "    random_matrix = random_matrix.view(B,-1)\n",
    "    \n",
    "    random_grasp = [random.randint(0,1) for _ in range(B)]\n",
    "    random_grasp = torch.tensor(random_grasp, dtype=torch.float32).view(B,-1).cuda()\n",
    "    \n",
    "    random_action = torch.cat((random_pose, random_matrix, random_grasp), 1)\n",
    "    return random_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Iter: 000100/150000, Cost: 37.93s, Load: 1.29, Forward: 11.85, Backward: 19.53, Loss: 0.010103\n",
      "===> Iter: 000200/150000, Cost: 37.23s, Load: 0.73, Forward: 11.71, Backward: 19.52, Loss: 0.010751\n",
      "===> Iter: 000300/150000, Cost: 37.14s, Load: 0.75, Forward: 11.68, Backward: 19.46, Loss: 0.009672\n",
      "===> Iter: 000400/150000, Cost: 37.57s, Load: 0.72, Forward: 11.86, Backward: 19.66, Loss: 0.010951\n",
      "===> Iter: 000500/150000, Cost: 37.24s, Load: 0.75, Forward: 11.70, Backward: 19.53, Loss: 0.012902\n",
      "===> Iter: 000600/150000, Cost: 37.47s, Load: 0.75, Forward: 11.86, Backward: 19.57, Loss: 0.011322\n",
      "===> Iter: 000700/150000, Cost: 37.47s, Load: 0.74, Forward: 11.83, Backward: 19.59, Loss: 0.011296\n",
      "===> Iter: 000800/150000, Cost: 39.90s, Load: 0.77, Forward: 12.97, Backward: 20.57, Loss: 0.012706\n",
      "===> Iter: 000900/150000, Cost: 39.72s, Load: 0.78, Forward: 12.90, Backward: 20.52, Loss: 0.009050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start train\n",
    "tic = time.time()\n",
    "end = time.time()\n",
    "trained_time = 0\n",
    "# max_iter = cfg.BASIC.MAX_EPOCH * len(train_dataloader)\n",
    "max_iter = cfg.BASIC.MAX_ITER\n",
    "time_dict = Time_dict()\n",
    "load_start = time.time()\n",
    "\n",
    "start_epoch = 0\n",
    "start_iter = 0\n",
    "\n",
    "value_network.train()\n",
    "prior.eval()\n",
    "pose_network.eval()\n",
    "conv_network.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "    \n",
    "for epoch in range(start_epoch, cfg.BASIC.MAX_EPOCH):\n",
    "    for iteration, inputs in enumerate(train_dataloader,1):\n",
    "        time_dict.load_data += time.time() - load_start\n",
    "        total_iteration = len(train_dataloader) * epoch + iteration\n",
    "            \n",
    "        # skip until start iter\n",
    "        if iteration < start_iter:\n",
    "            continue\n",
    "            \n",
    "        x = inputs[\"rgb\"].cuda()\n",
    "        pose = inputs[\"pose_xyz\"].cuda()\n",
    "        rotation = inputs[\"rotation_matrix\"].cuda()\n",
    "        grasp = inputs[\"grasp\"].cuda()\n",
    "\n",
    "        action = make_action(pose, rotation, grasp)\n",
    "        B,S,C,H,W = x.shape\n",
    "\n",
    "        # initialize the hidden state.\n",
    "        prior.hidden = prior.init_hidden()\n",
    "\n",
    "        reward_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # forward\n",
    "        with Timer() as t:\n",
    "            h_conv = encoder(x[:,0])\n",
    "            h_conv, skip = h_conv\n",
    "            B,C,H,W = h_conv.shape\n",
    "            h = h_conv.view(B, H*W*args.g_dim)\n",
    "\n",
    "            z_t, _, _ = prior(h)\n",
    "            z_t = z_t.view(cfg.BASIC.BATCH_SIZE, -1, 14, 14)\n",
    "\n",
    "            z_d_exp = pose_network(action[:,1].cuda()).detach()\n",
    "            h_pred_exp = conv_network(torch.cat([h_conv, z_t, z_d_exp], 1)).detach()\n",
    "            x_pred_exp = decoder([h_pred_exp, skip]).detach()\n",
    "\n",
    "            num_cand = 15\n",
    "            \n",
    "            for j in range(num_cand):\n",
    "                if j == 0:\n",
    "                    input_action = action[:,1].cuda()\n",
    "                else:\n",
    "                    input_action = make_random_action(pose[:,1], rotation[:,1], grasp[:,1])\n",
    "                value_network.zero_grad()\n",
    "                z_d_rand = pose_network(input_action.cuda())\n",
    "                h_pred_rand = conv_network(torch.cat([h_conv, z_t, z_d_rand], 1))\n",
    "                x_pred_rand = decoder([h_pred_rand, skip])\n",
    "                x_value_rand = value_network(x_pred_rand)\n",
    "\n",
    "                reward_label = []\n",
    "                for batch_idx in range(cfg.BASIC.BATCH_SIZE):\n",
    "                    reward_label.append(image_criterion(x_pred_rand[batch_idx], x_pred_exp[batch_idx].detach()).data)\n",
    "#                     print(reward_label[-1])\n",
    "                    \n",
    "                reward_label = torch.stack(reward_label)   \n",
    "                reward_label = Variable(reward_label, requires_grad=False)\n",
    "\n",
    "                reward_loss += train_loss.l1_criterion(x_value_rand, reward_label)\n",
    "        loss = reward_loss\n",
    "        train_loss.count += 1\n",
    "        \n",
    "        time_dict.forward += t.secs\n",
    "        \n",
    "        with Timer() as t:\n",
    "            loss.backward()\n",
    "        time_dict.backward += t.secs\n",
    "            \n",
    "        value_network_optimizer.step()\n",
    "        \n",
    "        # time setting\n",
    "        trained_time += time.time() - end\n",
    "        end = time.time() \n",
    "        \n",
    "        # save and print log\n",
    "        if total_iteration % args.log_step == 0:\n",
    "            log = train_loss.get_log()\n",
    "            eta_seconds = int((trained_time / total_iteration) * (max_iter - total_iteration))\n",
    "            \n",
    "            if (args.log2wandb) and (total_iteration % (args.log_step * 10)):\n",
    "                wandb.log(log,step=total_iteration)\n",
    "            \n",
    "            # print(threading.active_count())\n",
    "            print('===> Iter: {:06d}/{:06d}, Cost: {:.2f}s, Load: {:.2f}, Forward: {:.2f}, Backward: {:.2f}, Loss: {:.6f}'.format(total_iteration, \n",
    "                max_iter,  time.time() - tic, \n",
    "                time_dict.load_data, time_dict.forward, time_dict.backward, log[\"Model based BC train/loss value net\"]))\n",
    "            \n",
    "            train_loss.reset_log()\n",
    "            tic = time.time()\n",
    "            time_dict.reset()\n",
    "        \n",
    "        # save checkpoint\n",
    "        if total_iteration % args.save_step == 0:\n",
    "            checkpoint_dir = os.path.join(model_path,'checkpoint_iter{}'.format(total_iteration))\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            \n",
    "            value_network_path = os.path.join(checkpoint_dir, 'value_network.pth')\n",
    "            save_checkpoint(value_network, value_network_optimizer, epoch, iteration, value_network_path)\n",
    "            \n",
    "        # validation\n",
    "        if total_iteration % args.eval_step == 0:\n",
    "            print('validation start')\n",
    "            for iteration, inputs in enumerate(val_dataloader, 1):\n",
    "                with torch.no_grad():\n",
    "                    x = inputs[\"rgb\"].cuda()\n",
    "                    pose = inputs[\"pose_xyz\"].cuda()\n",
    "                    rotation = inputs[\"rotation_matrix\"].cuda()\n",
    "                    grasp = inputs[\"grasp\"].cuda()\n",
    "\n",
    "                    action = make_action(pose, rotation, grasp)\n",
    "                    B,S,C,H,W = x.shape\n",
    "\n",
    "                    # initialize the hidden state.\n",
    "                    prior.hidden = prior.init_hidden()\n",
    "                    \n",
    "                    h_conv = encoder(x[:,0])\n",
    "                    h_conv, skip = h_conv\n",
    "                    B,C,H,W = h_conv.shape\n",
    "                    h = h_conv.view(B, H*W*args.g_dim)\n",
    "\n",
    "                    z_t, _, _ = prior(h)\n",
    "                    z_t = z_t.view(cfg.BASIC.BATCH_SIZE, -1, 14, 14)\n",
    "\n",
    "                    z_d_exp = pose_network(action[:,1].cuda()).detach()\n",
    "                    h_pred_exp = conv_network(torch.cat([h_conv, z_t, z_d_exp], 1)).detach()\n",
    "                    x_pred_exp = decoder([h_pred_exp, skip]).detach()\n",
    "            \n",
    "                    for j in range(num_cand):\n",
    "                        if j == 0:\n",
    "                            input_action = action[:,1].cuda()\n",
    "                        else:\n",
    "                            input_action = make_random_action(pose[:,1], rotation[:,1], grasp[:,1])\n",
    "                        value_network.zero_grad()\n",
    "                        z_d_rand = pose_network(input_action.cuda())\n",
    "                        h_pred_rand = conv_network(torch.cat([h_conv, z_t, z_d_rand], 1))\n",
    "                        x_pred_rand = decoder([h_pred_rand, skip])\n",
    "                        x_value_rand = value_network(x_pred_rand)\n",
    "\n",
    "                        reward_label = []\n",
    "                        for batch_idx in range(cfg.BASIC.BATCH_SIZE):\n",
    "                            reward_label.append(image_criterion(x_pred_rand[batch_idx], x_pred_exp[batch_idx].detach()).data)\n",
    "                            \n",
    "                        reward_label = torch.stack(reward_label)   \n",
    "                        reward_label = Variable(reward_label, requires_grad=False)\n",
    "\n",
    "                        reward_loss += val_loss.l1_criterion(x_value_rand, reward_label)\n",
    "                    val_loss.count += 1\n",
    "\n",
    "                    if iteration >= 100:\n",
    "                        break\n",
    "            \n",
    "            val_log = val_loss.get_log()\n",
    "            if args.log2wandb:\n",
    "                wandb.log(val_log,step=total_iteration)\n",
    "            \n",
    "            print('===> Iter: {:06d}/{:06d}, VAL Loss: {:.6f}'.format(total_iteration, max_iter, val_log['Model based BC val/loss']))\n",
    "            print('')\n",
    "            val_loss.reset_log()        \n",
    "\n",
    "        load_start = time.time()\n",
    "\n",
    "        if total_iteration == cfg.BASIC.MAX_ITER:\n",
    "            sys.exit()\n",
    "\n",
    "    train_dataset.update_seed()\n",
    "    print(\"seed: {}\".format(train_dataset.seed))\n",
    "    start_iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_len': tensor([1]), 'index_list': [tensor([13598]), tensor([13599])], 'rgb': tensor([[[[[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "           [0.0000, 0.0000, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n",
      "           [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0039, 0.0000],\n",
      "           ...,\n",
      "           [0.7922, 0.8157, 0.8039,  ..., 0.8000, 0.7961, 0.7961],\n",
      "           [0.8157, 0.8157, 0.8000,  ..., 0.7843, 0.7725, 0.7765],\n",
      "           [0.8471, 0.8039, 0.7882,  ..., 0.7961, 0.8157, 0.8392]],\n",
      "\n",
      "          [[0.2549, 0.2392, 0.2118,  ..., 0.2314, 0.2353, 0.2314],\n",
      "           [0.2588, 0.2353, 0.2118,  ..., 0.2353, 0.2314, 0.2275],\n",
      "           [0.2510, 0.2235, 0.2157,  ..., 0.2314, 0.2353, 0.2353],\n",
      "           ...,\n",
      "           [0.6078, 0.6549, 0.6431,  ..., 0.6706, 0.6510, 0.6627],\n",
      "           [0.6431, 0.6549, 0.6353,  ..., 0.6235, 0.6235, 0.6078],\n",
      "           [0.7098, 0.6314, 0.6196,  ..., 0.6392, 0.6902, 0.6941]],\n",
      "\n",
      "          [[0.2627, 0.2549, 0.2314,  ..., 0.2471, 0.2471, 0.2431],\n",
      "           [0.2667, 0.2510, 0.2314,  ..., 0.2471, 0.2471, 0.2471],\n",
      "           [0.2627, 0.2431, 0.2353,  ..., 0.2431, 0.2471, 0.2431],\n",
      "           ...,\n",
      "           [0.3059, 0.3373, 0.3216,  ..., 0.3686, 0.3490, 0.3569],\n",
      "           [0.3216, 0.3333, 0.3216,  ..., 0.3137, 0.3255, 0.3059],\n",
      "           [0.3765, 0.3333, 0.3137,  ..., 0.3412, 0.3843, 0.3804]]],\n",
      "\n",
      "\n",
      "         [[[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0039, 0.0078],\n",
      "           [0.0078, 0.0000, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n",
      "           [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "           ...,\n",
      "           [0.7804, 0.8000, 0.8118,  ..., 0.8039, 0.8078, 0.8039],\n",
      "           [0.8196, 0.8235, 0.8039,  ..., 0.7765, 0.7686, 0.7804],\n",
      "           [0.8431, 0.8039, 0.7843,  ..., 0.8078, 0.8235, 0.8471]],\n",
      "\n",
      "          [[0.2549, 0.2353, 0.2078,  ..., 0.2314, 0.2471, 0.2392],\n",
      "           [0.2627, 0.2392, 0.2157,  ..., 0.2353, 0.2314, 0.2314],\n",
      "           [0.2588, 0.2353, 0.2353,  ..., 0.2275, 0.2157, 0.2431],\n",
      "           ...,\n",
      "           [0.6118, 0.6471, 0.6353,  ..., 0.6667, 0.6392, 0.6706],\n",
      "           [0.6471, 0.6588, 0.6392,  ..., 0.6314, 0.6157, 0.6039],\n",
      "           [0.6980, 0.6314, 0.6196,  ..., 0.6510, 0.6824, 0.6980]],\n",
      "\n",
      "          [[0.2706, 0.2392, 0.2314,  ..., 0.2510, 0.2549, 0.2471],\n",
      "           [0.2667, 0.2471, 0.2353,  ..., 0.2471, 0.2510, 0.2431],\n",
      "           [0.2784, 0.2431, 0.2392,  ..., 0.2431, 0.2471, 0.2431],\n",
      "           ...,\n",
      "           [0.2863, 0.3373, 0.3294,  ..., 0.3765, 0.3490, 0.3608],\n",
      "           [0.3176, 0.3412, 0.3333,  ..., 0.3176, 0.3294, 0.3137],\n",
      "           [0.3647, 0.3216, 0.3255,  ..., 0.3412, 0.3922, 0.3804]]]]]), 'heatmap': tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           ...,\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           ...,\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.]]]]]), 'pose_xyz': tensor([[[-0.3150, -0.3249,  1.6800],\n",
      "         [-0.3154, -0.3239,  1.6782]]]), 'pose_xyz_mask': tensor([[[1, 1, 1],\n",
      "         [1, 1, 1]]]), 'rotation_matrix': tensor([[[[ 0.9574,  0.1759, -0.2291],\n",
      "          [-0.2886,  0.6113, -0.7369],\n",
      "          [ 0.0104,  0.7716,  0.6360]],\n",
      "\n",
      "         [[ 0.9644,  0.1263, -0.2323],\n",
      "          [-0.2594,  0.6220, -0.7388],\n",
      "          [ 0.0512,  0.7727,  0.6327]]]]), 'grasp': tensor([[0., 0.]]), 'uv': tensor([[[[193.9401, 196.0149]],\n",
      "\n",
      "         [[194.1007, 195.8699]]]], dtype=torch.float64), 'uv_mask': tensor([[[[1., 1.]],\n",
      "\n",
      "         [[1., 1.]]]]), 'mtx': tensor([[[-351.6771,    0.0000,  128.0000],\n",
      "         [   0.0000, -351.6771,  128.0000],\n",
      "         [   0.0000,    0.0000,    1.0000]]], dtype=torch.float64), 'inv_mtx': tensor([[[-0.0028, -0.0000,  0.3640],\n",
      "         [-0.0000, -0.0028,  0.3640],\n",
      "         [ 0.0000,  0.0000,  1.0000]]], dtype=torch.float64), 'valid_sequence_mask': tensor([[1.]]), 'depth': tensor([[[[[3.9160, 3.9160, 3.9160,  ..., 3.9160, 3.9160, 3.9160],\n",
      "           [3.9219, 3.9219, 3.9219,  ..., 3.9219, 3.9219, 3.9219],\n",
      "           [3.9258, 3.9258, 3.9258,  ..., 3.9258, 3.9258, 3.9258],\n",
      "           ...,\n",
      "           [1.5146, 1.5146, 1.5146,  ..., 1.5146, 1.5146, 1.5146],\n",
      "           [1.5098, 1.5098, 1.5098,  ..., 1.5098, 1.5098, 1.5098],\n",
      "           [1.5039, 1.5039, 1.5039,  ..., 1.5039, 1.5039, 1.5039]]],\n",
      "\n",
      "\n",
      "         [[[3.9160, 3.9160, 3.9160,  ..., 3.9160, 3.9160, 3.9160],\n",
      "           [3.9219, 3.9219, 3.9219,  ..., 3.9219, 3.9219, 3.9219],\n",
      "           [3.9258, 3.9258, 3.9258,  ..., 3.9258, 3.9258, 3.9258],\n",
      "           ...,\n",
      "           [1.5146, 1.5146, 1.5146,  ..., 1.5146, 1.5146, 1.5146],\n",
      "           [1.5098, 1.5098, 1.5098,  ..., 1.5098, 1.5098, 1.5098],\n",
      "           [1.5039, 1.5039, 1.5039,  ..., 1.5039, 1.5039, 1.5039]]]]]), 'trajectory': tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           ...,\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           ...,\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "           [0., 0., 0.,  ..., 0., 0., 0.]]]]])}\n"
     ]
    }
   ],
   "source": [
    "for i, inputs in enumerate(train_dataloader,1):\n",
    "    print(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
