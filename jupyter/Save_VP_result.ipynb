{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kornia requires version >= 3.6. your version 3.6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import kornia\n",
    "sys.path.append('../')\n",
    "\n",
    "from pycode.dataset import RLBench_dataset_VP, imageaug_full_transform, train_val_split\n",
    "from pycode.config import _C as cfg\n",
    "from pycode.model.VideoPrediction import VIDEO_HOURGLASS, Discriminator\n",
    "from pycode.loss.vp_loss import Test_Loss_Video\n",
    "from pycode.misc import build_dataset_VP, str2bool, save_args, save_checkpoint, load_checkpoint\n",
    "from pycode.misc import make_overlay_image, make_overlay_image_and_heatmap, make_pos_image, convert_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configration file ../configs/RLBench_test_VP.yaml\n"
     ]
    }
   ],
   "source": [
    "# parser\n",
    "parser = argparse.ArgumentParser(description='parser for image generator')\n",
    "parser.add_argument('--config_file', type=str, default='', metavar='FILE', help='if not specified, use chepoint_path')\n",
    "parser.add_argument('--output_dirname', type=str, default='', help='if not specified, use checkpoint_path')\n",
    "parser.add_argument('--checkpoint_path','-c', type=str, help='e.g. output/RLdata/VP_pcf_dis_random/model_log/checkpoint_epoch0_iter100000/')\n",
    "args = parser.parse_args(args=['--config_file','../configs/RLBench_test_VP.yaml', '--output_dirname','result'])\n",
    "\n",
    "# get cfg data\n",
    "if len(args.config_file) > 0:\n",
    "    print('Loaded configration file {}'.format(args.config_file))\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "else:\n",
    "    config_dir = os.path.abspath(os.path.join(args.checkpoint_path, '../../'))\n",
    "    file_list = os.listdir(config_dir)\n",
    "    print(file_list)\n",
    "    yaml_file_name = [x for x in file_list if '.yaml' in x]\n",
    "    print(yaml_file_name)\n",
    "    config_file_path = os.path.join(config_dir, yaml_file_name[0])\n",
    "    print('Loaded configration file {}'.format(config_file_path))\n",
    "    cfg.merge_from_file(config_file_path)\n",
    "\n",
    "# define output dirname\n",
    "if len(args.output_dirname) > 0:\n",
    "    output_dirname = args.output_dirname\n",
    "    output_dirname = os.path.join(cfg.BASIC.OUTPUT_DIR, cfg.DATASET.NAME, output_dirname)\n",
    "else:\n",
    "    output_dirname = os.path.abspath(os.path.join(args.checkpoint_path,\"../../\"))\n",
    "\n",
    "cfg.PRED_LEN = 1\n",
    "cfg.BASIC.NUM_GPU = 1\n",
    "cfg.BASIC.BATCH_SIZE = 1\n",
    "\n",
    "# make output dir\n",
    "os.makedirs(cfg.BASIC.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# difine save model path\n",
    "model_path = os.path.join(cfg.BASIC.OUTPUT_DIR, 'model_log')\n",
    "\n",
    "# set seed and cuda\n",
    "torch.manual_seed(cfg.BASIC.SEED)\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(cfg.BASIC.DEVICE)\n",
    "\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(cfg.BASIC.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of future is 1 frame\n",
      "load json data\n",
      "fix parallel\n",
      "fix parallel\n"
     ]
    }
   ],
   "source": [
    "# set dataset\n",
    "test_dataset = build_dataset_VP(cfg, mode='test')\n",
    "\n",
    "# set dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=cfg.BASIC.BATCH_SIZE, shuffle=False, num_workers=cfg.BASIC.WORKERS)\n",
    "\n",
    "# set model 1\n",
    "cfg.VIDEO_HOUR.MODE = 'pcf'\n",
    "model1 = VIDEO_HOURGLASS(cfg)\n",
    "video_checkpoint_path1 = '../output/RLBench3/VP_pcf_PickUpCup/model_log/checkpoint_iter100000/vp.pth'\n",
    "model1, _, _, _, _ = load_checkpoint(model1, video_checkpoint_path1, fix_parallel=True)\n",
    "model1 = torch.nn.DataParallel(model1, device_ids = list(range(cfg.BASIC.NUM_GPU)))\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# set model 2\n",
    "cfg.VIDEO_HOUR.MODE = 'pc'\n",
    "model2 = VIDEO_HOURGLASS(cfg)\n",
    "video_checkpoint_path2 = '../output/RLBench3/VP_pc_PickUpCup/model_log/checkpoint_iter100000/vp.pth'\n",
    "model2, _, _, _, _ = load_checkpoint(model2, video_checkpoint_path2, fix_parallel=True)\n",
    "model2 = torch.nn.DataParallel(model2, device_ids = list(range(cfg.BASIC.NUM_GPU)))\n",
    "model2 = model2.to(device)\n",
    "\n",
    "start_epoch, start_iter = 0, 1\n",
    "\n",
    "def make_videomodel_input(inputs, device, sequence_id=0, mode='pcf'):\n",
    "    '''\n",
    "    output:\n",
    "    dictionary{\n",
    "    rgb => torch.Tensor shape=(B,S,C,H,W),\n",
    "    pose => torch.Tensor shape=(B,S,C,H,W)}\n",
    "    '''\n",
    "    if mode == 'pcf':\n",
    "        index_list = [sequence_id, sequence_id+1, sequence_id+3]\n",
    "        rgb = inputs['rgb'][:,index_list].to(device)\n",
    "        pose_heatmap = inputs['pose'][:,:4].to(device)\n",
    "        pose_xyz = inputs['pose_xyz'][:,:4].to(device)\n",
    "        rotation_matrix = inputs['rotation_matrix'][:,:4].to(device)\n",
    "        grasp = inputs['grasp'][:,:4].to(device)\n",
    "        if cfg.VIDEO_HOUR.INPUT_DEPTH:\n",
    "            depth = inputs['depth'][:,index_list].to(device)\n",
    "    elif mode == 'pc':\n",
    "        index_list = [sequence_id, sequence_id+1]\n",
    "        rgb = inputs['rgb'][:,index_list].to(device)\n",
    "        pose_heatmap = inputs['pose'][:,:3].to(device)\n",
    "        pose_xyz = inputs['pose_xyz'][:,:3].to(device)\n",
    "        rotation_matrix = inputs['rotation_matrix'][:,:3].to(device)\n",
    "        grasp = inputs['grasp'][:,:3].to(device)\n",
    "        if cfg.VIDEO_HOUR.INPUT_DEPTH:\n",
    "            depth = inputs['depth'][:,index_list].to(device)\n",
    "    elif mode == 'c':\n",
    "        rgb = inputs['rgb'][:,1].to(device)\n",
    "        pose_heatmap = inputs['pose'][:,1:3].to(device)\n",
    "        pose_xyz = inputs['pose_xyz'][:,1:3].to(device)\n",
    "        rotation_matrix = inputs['rotation_matrix'][:,1:3].to(device)\n",
    "        grasp = inputs['grasp'][:,1:3].to(device)\n",
    "        if cfg.VIDEO_HOUR.INPUT_DEPTH:\n",
    "            depth = inputs['depth'][:,1].to(device)\n",
    "    \n",
    "    input_dict = {}\n",
    "    input_dict['rgb'] = rgb\n",
    "    input_dict['pose'] = pose_heatmap\n",
    "    input_dict['pose_xyz'] = pose_xyz\n",
    "    input_dict['rotation_matrix'] = rotation_matrix\n",
    "    input_dict['grasp'] = grasp\n",
    "    if cfg.VIDEO_HOUR.INPUT_DEPTH:\n",
    "        input_dict['depth'] = depth\n",
    "\n",
    "    return input_dict\n",
    "\n",
    "def save_diff_heatmap_overlay(pred, gt):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    totensor = torchvision.transforms.ToTensor()\n",
    "    heatmap = torch.abs(pred - gt).squeeze(1)\n",
    "\n",
    "    for i in range(heatmap.shape[0]):\n",
    "        image_ins = pred[i].cpu().numpy().transpose((1, 2, 0))*255\n",
    "        image_ins = image_ins.clip(0, 255).astype(np.uint8)\n",
    "        \n",
    "        heatmap_ins = torch.mean(heatmap[i],dim=0).cpu().numpy()*255\n",
    "        heatmap_ins = heatmap_ins.clip(0, 255)\n",
    "        heatmap_ins  = 255 - heatmap_ins.astype(np.uint8)\n",
    "\n",
    "        heatmap_ins = cv2.applyColorMap(heatmap_ins, cv2.COLORMAP_JET)\n",
    "        overlayed_image = cv2.addWeighted(heatmap_ins, 0.6, image_ins, 0.4, 0)\n",
    "        overlayed_image = totensor(overlayed_image.transpose((0,1,2)))\n",
    "\n",
    "        if i == 0:\n",
    "            overlayed_image_batch = torch.unsqueeze(overlayed_image,0)\n",
    "        else:\n",
    "            overlayed_image_batch = torch.cat((overlayed_image_batch, torch.unsqueeze(overlayed_image,0)), 0)\n",
    "    return overlayed_image_batch\n",
    "\n",
    "calc_psnr = kornia.losses.psnr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab64091aecf40f9a2fa647b61384e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4965.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(model_path,'image')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "iteration = 0\n",
    "for inputs in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        gt_image = inputs['rgb'][:,2].to(device)\n",
    "        outputs1 = model1(make_videomodel_input(inputs,device,mode='pcf'))\n",
    "        output_image1 = outputs1['rgb']\n",
    "        PSNR1 = calc_psnr(torch.clamp(output_image1*255,0,255),gt_image*255, 255)\n",
    "        \n",
    "        outputs2 = model2(make_videomodel_input(inputs,device,mode='pc'))\n",
    "        output_image2 = outputs2['rgb']\n",
    "        PSNR2 = calc_psnr(torch.clamp(output_image2*255,0,255),gt_image*255, 255)\n",
    "        if PSNR1 - PSNR2 > 0.5:\n",
    "            diff_image_gt = save_diff_heatmap_overlay(gt_image,gt_image)\n",
    "            diff_image1 = save_diff_heatmap_overlay(output_image1,gt_image)\n",
    "            diff_image2 = save_diff_heatmap_overlay(output_image2, gt_image)\n",
    "            save_image = torch.cat((gt_image.cpu(),output_image1.cpu(),output_image2.cpu(),diff_image_gt, diff_image1, diff_image2),0)\n",
    "            diff_psnr = PSNR1 - PSNR2\n",
    "            # save output image\n",
    "            save_image_path = os.path.join(checkpoint_dir,'train_image_output_{}_{}.jpg'.format(iteration,diff_psnr.cpu().item()))\n",
    "            torchvision.utils.save_image(save_image, save_image_path,nrow=3)\n",
    "    \n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
